{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing Student Feedback Using Artificial Intelligence techniques in Python\n"
      ],
      "metadata": {
        "id": "YcY0RVNh9DUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Library Pakages\n"
      ],
      "metadata": {
        "id": "KtqwrVvk9JPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas vaderSentiment googletrans==4.0.0-rc1 nltk scikit-learn langdetect gensim"
      ],
      "metadata": {
        "id": "6TF1sgcY9bin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libary Pakckages"
      ],
      "metadata": {
        "id": "X_pAbj3s-Mt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "\n",
        "import nltk\n",
        "# Download the NLTK words corpus if not already downloaded\n",
        "nltk.download('words')\n",
        "# Tokenize and preprocess the feedback data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from gensim import corpora, models\n",
        "\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "import spacy\n",
        "# Load the spaCy model (you may need to download it first)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XnfixGR4-d0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Global functions"
      ],
      "metadata": {
        "id": "DJB3XOOaAj_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_excel_file(path):\n",
        "  '''Read excel file and retun a pandas dataframe'''\n",
        "\n",
        "  try:\n",
        "      # Read the Excel file into a DataFrame\n",
        "      df = pd.read_excel(path)\n",
        "\n",
        "      # You can now work with the data in the DataFrame\n",
        "      print(\"Data imported successfully:\")\n",
        "      print(df.head())  # Print the first few rows of the DataFrame\n",
        "      return df\n",
        "  except FileNotFoundError:\n",
        "      print(f\"File not found at path: {path}\")\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "  '''Filter text for tokenization'''\n",
        "\n",
        "  try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokenized_text = word_tokenize(text.lower())\n",
        "    filtered_text = [word for word in tokenized_text if word.isalpha() and word not in stop_words]\n",
        "    return filtered_text\n",
        "  except :\n",
        "    pass\n",
        "\n",
        "\n",
        "def clean_text_questions(text):\n",
        "  '''Clean text to keep content'''\n",
        "\n",
        "  pattern = r'CLO\\s*:\\s*(-\\s*\\d+)?'\n",
        "\n",
        "  # Remove the patterns\n",
        "  cleaned_text = re.sub(pattern, '', text)\n",
        "\n",
        "  return cleaned_text\n",
        "\n",
        "\n",
        "def detect_and_translate(text):\n",
        "  '''Detect text language and if it is arabic , transalte it to english. Otherswise leave it as it is'''\n",
        "\n",
        "  # Detect the language of the input text\n",
        "  detected_language = detect(text)\n",
        "\n",
        "  # If the detected language is Arabic, translate to English\n",
        "  if detected_language == 'ar':\n",
        "      translator = Translator()\n",
        "      translator.raise_Exception = True\n",
        "      translated_text = translator.translate(text, src='ar', dest='en')\n",
        "      return translated_text.text\n",
        "  else:\n",
        "      return text\n",
        "\n",
        "\n",
        "def keep_real_english_words(text):\n",
        "  '''Keep English words to capture meaningful words and get rid of strange responses'''\n",
        "\n",
        "  try:\n",
        "    # Tokenize the text into words\n",
        "    words_in_text = word_tokenize(text)\n",
        "\n",
        "    # Get the set of real English words\n",
        "    english_word_set = set(words.words())\n",
        "\n",
        "    # Keep only real English words\n",
        "    real_english_words = [word for word in words_in_text if word.lower() in english_word_set]\n",
        "\n",
        "    # Join the remaining real English words back into a text string\n",
        "    cleaned_text = ' '.join(real_english_words)\n",
        "\n",
        "    return cleaned_text\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "def sentiment_VADER(text):\n",
        "  '''Performing sentiment analysis without labeled data'''\n",
        "\n",
        "  # Initialize the VADER sentiment analyzer\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "  try:\n",
        "    # Analyze sentiment\n",
        "    sentiment_scores = analyzer.polarity_scores(text)\n",
        "\n",
        "    # Determine sentiment based on scores\n",
        "    compound_score = sentiment_scores['compound']\n",
        "\n",
        "    if compound_score >= 0.05:\n",
        "      sentiment = \"Positive\"\n",
        "    elif compound_score <= -0.05:\n",
        "      sentiment = \"Negative\"\n",
        "    else:\n",
        "      sentiment = \"Neutral\"\n",
        "\n",
        "    return sentiment\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "def sentiment_spaCy(text):\n",
        "  '''Sentiment analysis using pre-trained spaCy models.'''\n",
        "  try:\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Calculate the sentiment score based on word vectors\n",
        "    sentiment_score = doc.sentiment\n",
        "\n",
        "    if sentiment_score >= 0.2:\n",
        "        return \"Positive\"\n",
        "    elif sentiment_score <= -0.2:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "Oe7rqAj0AqPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i0m8fDHtwMjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Datasets"
      ],
      "metadata": {
        "id": "GlYFze4N_KRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments = read_excel_file('AI_Engineer_Dataset_Task_1.xlsx')\n",
        "cources = read_excel_file('AI_Engineer_Dataset_Task_2.xlsx')"
      ],
      "metadata": {
        "id": "oMXrHhGv_Num"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre processing texts"
      ],
      "metadata": {
        "id": "yvwFHR4WEzGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill null values with 'NOS' -> No String"
      ],
      "metadata": {
        "id": "zXDSpVyPE9FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments[['QuestionText', 'ParticipantResponse']] = comments[['QuestionText', 'ParticipantResponse']].fillna('NOS')"
      ],
      "metadata": {
        "id": "64hvIWUEtBvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove special pattern in text"
      ],
      "metadata": {
        "id": "BY3HcfPKwNlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the clean function to the text columns\n",
        "comments['QuestionText'] = comments['QuestionText'].apply(clean_text_questions)"
      ],
      "metadata": {
        "id": "bG6kym7rJPQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translate text if it is not english"
      ],
      "metadata": {
        "id": "kCBbZQ5GPdJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the translate function to the text columns\n",
        "comments['QuestionText'] = comments['QuestionText'].apply(detect_and_translate)\n",
        "comments['ParticipantResponse'] = comments['ParticipantResponse'].apply(detect_and_translate)\n"
      ],
      "metadata": {
        "id": "0uzfyWJtO0jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep meaningful english words"
      ],
      "metadata": {
        "id": "grbU5dY3dcxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments['ParticipantResponse'] = comments['ParticipantResponse'].apply(keep_real_english_words)"
      ],
      "metadata": {
        "id": "r1cUAJVDdamH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "VGV3KfIgCZnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new column as sentiment of text\n",
        "comments['ResponseSentiment_VADER'] = comments['ParticipantResponse'].apply(sentiment_VADER)\n",
        "comments['ResponseSentiment_spaCy'] = comments['ParticipantResponse'].apply(sentiment_spaCy)\n"
      ],
      "metadata": {
        "id": "JtUjNi6pbEVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Topic Modeling"
      ],
      "metadata": {
        "id": "K7Jd8k80CkFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feedback data\n",
        "\n",
        "feedbacks = comments['ParticipantResponse'].tolist()\n",
        "tokenized_feedbacks = [preprocess_text(feedback) for feedback in feedbacks if feedback]\n",
        "\n",
        "# Create a dictionary and corpus for LDA\n",
        "dictionary = corpora.Dictionary(tokenized_feedbacks)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_feedbacks]\n",
        "\n",
        "# Apply LDA model\n",
        "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print the identified topics and their keywords\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(f\"Topic {topic[0]}: {topic[1]}\")\n",
        "\n",
        "# Get the dominant topic for each feedback\n",
        "for i, feedback in enumerate(feedbacks):\n",
        "    if not feedback:\n",
        "        continue  # Skip null or empty feedback\n",
        "    bow = dictionary.doc2bow(tokenized_feedbacks[i])\n",
        "    dominant_topic = max(lda_model[bow], key=lambda x: x[1])\n",
        "    print(f\"Feedback {i + 1}: Dominant Topic - {dominant_topic[0]}, Probability - {dominant_topic[1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "itjJV9uhrwub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}